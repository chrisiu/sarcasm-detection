# -*- coding: utf-8 -*-
"""bert-base-uncased.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nH7SksacCYfFUymnkwmGmL8P8Fh5eLw8

# Bert Base Uncased Test

Imports
"""

!pip install transformers==4.44.2 accelerate==1.1.1 datasets torch gradio lime scikit-learn numpy

import torch
from datasets import load_dataset, concatenate_datasets
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from lime.lime_text import LimeTextExplainer
import numpy as np

import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

"""Loading dataset from huggingface (https://huggingface.co/datasets/raquiba/Sarcasm_News_Headline)"""

dataset = load_dataset("raquiba/Sarcasm_News_Headline")

dataset = dataset.rename_column("headline", "text")
dataset = dataset.rename_column("is_sarcastic", "label")

dataset = dataset.remove_columns(["article_link"])

dataset

"""Train/test split"""

dataset = concatenate_datasets([dataset["train"], dataset["test"]])
dataset = dataset.train_test_split(test_size=0.2, seed=42)

train = dataset["train"]
test = dataset["test"]

train, test

"""Choosing Tokenizer and Tokenizing Data"""

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def preprocess(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, max_length=128)

train_dataset = train.map(preprocess, batched=True)
test_dataset = test.map(preprocess, batched=True)

"""Load in Model"""

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

"""Training Arguments"""

training_args = TrainingArguments(
    output_dir="./sarcasm_model",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none",
    save_total_limit=2
)

collator = DataCollatorWithPadding(tokenizer)

"""Creating Function for Metrics"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary'
    )

    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

"""Train the Model"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

device = (
    "cuda" if torch.cuda.is_available()
    else "mps" if torch.backends.mps.is_available()
    else "cpu"
)

trainer.train()

"""Evaluation"""

eval_results = trainer.evaluate()
for key, value in eval_results.items():
    if isinstance(value, float):
        print(f"{key}: {value:.4f}")
    else:
        print(f"{key}: {value}")

"""### Testing Examples with LIME (via the Onion)

Sarcastic Examples
"""

import torch
from lime.lime_text import LimeTextExplainer

model_cpu = model.to("cpu")

def predict_proba(texts):
    inputs = tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )

    inputs = {k: v.to("cpu") for k, v in inputs.items()}

    with torch.no_grad():
        logits = model_cpu(**inputs).logits

    return torch.softmax(logits, dim=1).cpu().numpy()


explainer = LimeTextExplainer(class_names=["not_sarcastic", "sarcastic"])

sample = "Vatican Formally Recognizes First Gen Z Demon"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()

sample = "Study Finds Processed Meats Carcinogenic But They Were On Sale"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()

sample = "FDA Approves New Drug  That Reverses Effects Of Narcan"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()

"""Non-Sarcastic Examples (via AP)"""

sample = "FAA launches investigation into US airlines over flight cuts ordered during the shutdownn"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()

sample = "Billionaire spacewalker is back before the Senate seeking NASA’s top job"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()

sample = "Wall Street rises to the edge of its all-time high"

explanation = explainer.explain_instance(
    sample,
    predict_proba,
    num_features=10
)


explanation.show_in_notebook()